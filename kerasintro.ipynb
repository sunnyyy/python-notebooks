{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple neural-network with Keras\n",
    "\n",
    "This is a simple quick-start in performing image recognition in a neural network in Keras. Parts of the tutorial are adapted from Xavier Snelgrove at the University of Toronto.\n",
    "\n",
    "<a href=\"https://www.tensorflow.org\">TensorFlow</a> is a software library for efficient numerical computations, and is particularly well suited for deep neural networks. It includes several functionalities, such as running code on multiple CPUs/GPUs, and automatic gradient computations and training, among others. \n",
    "\n",
    "TensorFlow's computation model is a bit idiosyncratic. A computation is represented as a graph, where the nodes are operations, and edges are connections between operations. A graph must be created first, and then launched to \"lazily\" execute the computations. \n",
    "\n",
    "This computation model is very powerful, but has a steep learning curve. Therefore, instead of using TensorFlow directly, we'll use <a href=\"https://keras.io\">Keras</a>, a high-level interface to TF that is sufficiently powerful for the kind of neural nets we want to build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install prerequisites\n",
    "\n",
    "Type \n",
    "\n",
    "`pip install keras`\n",
    "\n",
    "to install Keras. Test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have issues, follow the instructions below to install in a virtual environment.\n",
    "\n",
    "### Configure a virtual environment\n",
    "\n",
    "Ignore this part if `keras` is installed.\n",
    "\n",
    "The `virtualenv` virtual environment basically encapsulates a full isolated Python environment. Install virtualenv thus:\n",
    "\n",
    "    pip install virtualenv\n",
    "\n",
    "Navigate to your home directory `cd ~` and create a virtual environment. We'll call it `kerasenv`\n",
    "\n",
    "    virtualenv kerasenv\n",
    "\n",
    "Now, to switch your shell environment to be within the env:\n",
    "\n",
    "    source kerasenv/bin/activate\n",
    "    \n",
    "Great: now you can install keras and tensorflow.\n",
    "\n",
    "    pip install tensorflow keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to build a neural network!\n",
    "First let's import some prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7,7) # Make the figures a bit bigger\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data\n",
    "\n",
    "First, we load the image data. Keras will download it and parse it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = 10\n",
    "\n",
    "# the data, shuffled and split between tran and test sets\n",
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "print(\"trainX original shape\", trainX.shape)\n",
    "print(\"trainy original shape\", trainy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some examples of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(trainX[i], interpolation='none')\n",
    "    plt.title(\"Class {}\".format(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural-network is going to take a single vector for each training example, so we need to reshape the input so that each image becomes a single 32x32x3 dimensional vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainX = trainX.reshape(trainX.shape[0], -1)\n",
    "testX = testX.reshape(testX.shape[0], -1)\n",
    "print(\"Training matrix shape\", trainX.shape)\n",
    "print(\"Testing matrix shape\", testX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the target matrices to be in the one-hot format, i.e.\n",
    "\n",
    "```\n",
    "0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "etc.\n",
    "```\n",
    "\n",
    "This is a common representation for many neural net packages. Each label becomes a vector in 10 dimensions (since 10 is the number of classes), with a 1 on the index corresponding to the label and zero everywhere else.\n",
    "\n",
    "The one-hot representation is nice for two reasons: it matches the shape of the output layer, and it's flexible enough to accommodate a *distribution* over labels for a data point rather than a single label -- a departure from the paradigms we've used in class!\n",
    "\n",
    "While our images only have one label each, you can imagine problems where multiple labels per datapoint are given.\n",
    "\n",
    "**Exercise**: What is an example of a dataset and task where each data-point may have more than one label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainy_onehot = keras.utils.to_categorical(trainy, num_classes=10)  # convert to one-hot\n",
    "testy_onehot = keras.utils.to_categorical(testy, num_classes=10)  \n",
    "\n",
    "print('First three labels of training')\n",
    "print(trainy[:3])  # show first three labels of training set\n",
    "print('First three labels of training in one-hot representation')\n",
    "print(trainy_onehot[:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification with Keras\n",
    "\n",
    "The code below implements multi-class logistic (softmax) regression. In other works, this is only the top layer of your PS5 network, with no hidden layer. This means it's a linear classifier (albeit with 10 hyperplanes, each represented by a neuron).\n",
    "\n",
    "The training method is `fit` (just like in `scikit-learn`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nn_softmax_model(eta, epochs, batch_size):\n",
    "    model = Sequential()   \n",
    "    model.add(Dense(10, \n",
    "                activation='softmax', \n",
    "                input_dim=trainX.shape[1]))  # the dim of the data that feeds to this layer\n",
    "                \n",
    "    sgd = optimizers.SGD(lr=eta)   # lr is the learning rate (eta)\n",
    "    model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',  # data loss function at output\n",
    "              metrics=['accuracy'])  # how to evaluate correctness\n",
    "\n",
    "    # Train the model using a given batch size and some number of epochs\n",
    "    model.fit(trainX, trainy_onehot, epochs=epochs, batch_size=batch_size, verbose = 0) \n",
    "    # return the trained model\n",
    "    return model\n",
    "\n",
    "softmax_classifier = nn_softmax_model(0.001, 10, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Evaluate this trained model on the test data, which produces a tuple consisting of the loss on the test set and the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = softmax_classifier.evaluate(testX, testy_onehot)\n",
    "print 'Accuracy on test set:', test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Single-Hidden Layer Model\n",
    "\n",
    "Now let's replicate your PS5 model (a hidden layer with rectified linear units) using Keras. We have a bit more flexibility with Keras than your model in that we can specify different alpha values for the regularizer at each layer, but in this example, we'll use the same alpha for all layers.\n",
    "\n",
    "The layers must be added in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nn_hidden1_model(H, eta, epochs, batch_size):\n",
    "    model = Sequential()  \n",
    "\n",
    "    # hidden relu layer\n",
    "    model.add(Dense(H,   # number of neurons H in this hidden layer\n",
    "                activation='relu', \n",
    "                name='hidden',  # can name layers for our convenience\n",
    "                input_dim=trainX.shape[1]))  # dim of the data that feeds to this layer\n",
    "                \n",
    "    # output softmax layer\n",
    "    model.add(Dense(10, \n",
    "                activation='softmax'))                \n",
    "\n",
    "    sgd = optimizers.SGD(lr=eta)    \n",
    "    model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',  # data loss function at output\n",
    "              metrics=['accuracy'])  # how to evaluate correctness\n",
    "\n",
    "    # Train the model using a given batch size and some number of epochs\n",
    "    model.fit(trainX, trainy_onehot, epochs=epochs, batch_size=batch_size, verbose=0)  \n",
    "    \n",
    "    return model\n",
    "\n",
    "hidden1_classifier = nn_hidden1_model(100, 0.001, 10, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this new model on the test data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = hidden1_classifier.evaluate(testX, testy_onehot)\n",
    "print 'Accuracy on test set:', test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Hidden-Layer Network with Dropout\n",
    "\n",
    "Now we'll do a 3 layer fully connected network. \n",
    "\n",
    "With three layers and so many neurons, even L2 regularization isn't enough to prevent overfitting. Instead, we'll use a technique called \"dropout\" for regularization,\n",
    "which freezes a random set of neurons on each gradient descent step, preventing them from being updated. It turns out this simple technique is an extremely effective form of regularization, so much so that we can even do away with the L2 regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nn_hidden2_model(H1, H2, eta, epochs, batch_size):\n",
    "    model = Sequential()  \n",
    "\n",
    "    # hidden relu layer\n",
    "    model.add(Dense(H1,   # number of neurons H in this hidden layer\n",
    "                activation='relu', \n",
    "                name='hidden1',  # can name layers for our convenience\n",
    "                input_dim=trainX.shape[1]))  # dim of the data that feeds to this layer\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # hidden relu layer\n",
    "    model.add(Dense(H2,   # number of neurons H in this hidden layer\n",
    "                activation='relu', \n",
    "               name='hidden2'))  # can name layers for our convenience\n",
    "    model.add(Dropout(0.2)) \n",
    "                \n",
    "    # output softmax layer\n",
    "    model.add(Dense(10, \n",
    "                activation='softmax'))                \n",
    "    \n",
    "    sgd = optimizers.SGD(lr=eta)    \n",
    "    model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',  # data loss function at output\n",
    "              metrics=['accuracy'])  # how to evaluate correctness\n",
    "\n",
    "    # Train the model using a given batch size and some number of epochs\n",
    "    model.fit(trainX, trainy_onehot, epochs=epochs, batch_size=batch_size, verbose=0)  \n",
    "    \n",
    "    return model\n",
    "\n",
    "hidden2_classifier = nn_hidden2_model(100, 100, 0.001, 10, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this new model on the test data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = hidden2_classifier.evaluate(testX, testy_onehot)\n",
    "print 'Accuracy on test set:', test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "Here is a convolutional network for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 200\n",
    "data_augmentation = True\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = optimizers.SGD(lr=eta)  \n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(trainX, trainy_onehot,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              shuffle=True)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(testX, testy_onehot)\n",
    "print 'Accuracy on test set:', test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
