{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The unreasonable effectiveness of Character-level Language Models\n",
    "## (and why RNNs are still cool)\n",
    "\n",
    "Adapted by Sravana Reddy from a [notebook](http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139) by [Yoav Goldberg](http://www.cs.biu.ac.il/~yogo).\n",
    "\n",
    "RNNs, LSTMs and Deep Learning are all the rage, and a recent [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy is doing a great job explaining what these models are and how to train them.\n",
    "It also provides some very impressive results of what they are capable of.  \n",
    "\n",
    "First, we download a sample of text from Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "shakespeare = urllib2.urlopen('http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt').read()\n",
    "print('Downloaded text with', len(shakespeare), 'characters')\n",
    "print('Showing the 100 characters in the text...')\n",
    "print(shakespeare[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-Based Character Model\n",
    "\n",
    "The code below uses Keras to train an LSTM character model. Run it if you have Keras working on your laptop. Read through the code otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up the LSTM architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "def make_model(H, maxlen, vocabsize):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(H, return_sequences = True, input_shape=(maxlen, vocabsize)))\n",
    "    model.add(TimeDistributed(Dense(vocabsize, activation='softmax')))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = RMSprop(lr=0.001), metrics=['accuracy'])\n",
    "    print('Created an LSTM model')\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the text data appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical  # for one-hot encoding\n",
    "import numpy as np\n",
    "\n",
    "def text_to_chardata(text, maxlen):\n",
    "    # get data into batches with characters mapped to indices\n",
    "    char2id = {}  \n",
    "    curid = 0\n",
    "    X = []\n",
    "    Y = []\n",
    "    X_sample = []\n",
    "    for i, c in enumerate(text):\n",
    "        if c not in char2id:\n",
    "            char2id[c] = curid\n",
    "            curid += 1\n",
    "        if len(X_sample)==maxlen:\n",
    "            X.append(X_sample)\n",
    "            Y.append(X_sample[1:]+[char2id[c]])  # output shifted by 1\n",
    "            X_sample = []\n",
    "        X_sample.append(char2id[c])\n",
    "    \n",
    "    # convert data into one-hot vectors\n",
    "    X = np.array(map(lambda X_sample: to_categorical(X_sample, num_classes=len(char2id)), X))\n",
    "    Y = np.array(map(lambda Y_sample: to_categorical(Y_sample, num_classes=len(char2id)), Y))\n",
    "    \n",
    "    return X, Y, char2id\n",
    "    \n",
    "# load the first million characters of the shakespeare data\n",
    "shakX, shakY, char2id = text_to_chardata(shakespeare[:1000000], 200)\n",
    "print('Data size')\n",
    "print(shakX.shape, shakY.shape)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shakmodel = make_model(100, shakX.shape[1], shakX.shape[2])\n",
    "shakmodel.fit(shakX, shakY, batch_size = 100, epochs = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the LSTM model above train while we talk about **non-neural language models**. Skip the block below for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(trained_model, H, char2id, outlen=1000):\n",
    "    print('Building Inference model...')\n",
    "    inference_model = Sequential()\n",
    "    # 1. The inference model only takes one sample in the batch, and it always has sequence length 1.\n",
    "    # 2. The inference model is stateful, meaning it inputs the output hidden state (\"its history state\")\n",
    "    #    to the next batch input.\n",
    "    inference_model.add(LSTM(H, batch_input_shape=(1, 1, len(char2id)), stateful = True))\n",
    "    # Since the above LSTM does not output sequences, we don't need TimeDistributed anymore.\n",
    "    inference_model.add(Dense(len(char2id), activation='softmax'))\n",
    "    # Copy the weights of the trained network. Both should have the same exact number of parameters (why?).\n",
    "    inference_model.set_weights(trained_model.get_weights())\n",
    "    \n",
    "    id2char = {v:k for k, v in char2id.items()}\n",
    "\n",
    "    inference_model.reset_states()  \n",
    "    startChar = np.zeros((1, 1, len(char2id)))\n",
    "    startChar[0, 0, 0] = 1\n",
    "\n",
    "    # sample\n",
    "    generated = []\n",
    "    for i in range(outlen):\n",
    "        nextCharProbs = inference_model.predict(startChar)\n",
    "    \n",
    "        nextCharProbs = np.asarray(nextCharProbs).astype('float64') \n",
    "        nextCharProbs = nextCharProbs / nextCharProbs.sum()  \n",
    "    \n",
    "        nextCharId = np.random.multinomial(1, nextCharProbs.squeeze(), 1).argmax()\n",
    "        generated.append(id2char[nextCharId]) \n",
    "        startChar.fill(0)\n",
    "        startChar[0, 0, nextCharId] = 1\n",
    "    \n",
    "    print ''.join(generated)\n",
    "\n",
    "generate(shakmodel, 100, char2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsmoothed Maximum Likelihood Character Level Language Model \n",
    "\n",
    "We want a model whose job is to guess the next character based on the previous n letters. For example, having seen `ello`, the next characer is likely to be either a commma or space (if we assume is is the end of the word \"hello\"), or the letter `w` if we believe we are in the middle of the word \"mellow\". Humans are quite good at this, but of course seeing a larger history makes things easier (if we were to see 5 letters instead of 4, the choice between space and `w` would have been much easier).\n",
    "\n",
    "We will call n, the number of letters we need to guess based on, the _order_ of the language model.\n",
    "\n",
    "RNNs and LSTMs can potentially learn infinite-order language model (they guess the next character based on a \"state\" which supposedly encode all the previous history). We here will restrict ourselves to a fixed-order language model.\n",
    "\n",
    "So, we are seeing `n` letters, and need to guess the `n+1`th one. We are also given a large-ish amount of text (say, all of Shakespear works) that we can use. How would we go about solving this task?\n",
    "\n",
    "Mathematiacally, we would like to learn a function `P(c | h)`. Here, `c` is a character, `h` is an `n`-letters history, and `P(c|h)` stands for how likely is it to see `c` after we've seen `h`.\n",
    "\n",
    "Perhaps the simplest approach would be to just count and divide (a.k.a **maximum likelihood estimates**). We will count the number of times each letter `c` appeared after `h`, and divide by the total numbers of letters appearing after `h`. The **unsmoothed** part means that if we did not see a given letter following `h`, we will just give it a probability of zero.\n",
    "\n",
    "And that's all there is to it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Code\n",
    "Here is the code for training the model. `fname` is a file to read the characters from. `order` is the history size to consult. Note that we pad the data with leading `~` so that we also learn how to start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import *\n",
    "\n",
    "def train_char_lm(text, order=4):\n",
    "    lm = defaultdict(Counter)\n",
    "    pad = [\"<s>\"] * order  # beginning of sentence tags at start\n",
    "    data = pad + list(text)\n",
    "    for i in xrange(len(data)-order):\n",
    "        history, char = data[i:i+order], data[i+order]\n",
    "        lm[tuple(history)][char]+=1\n",
    "    def normalize(counter):\n",
    "        s = float(sum(counter.values()))\n",
    "        return [(c,cnt/s) for c,cnt in counter.iteritems()]\n",
    "    outlm = {hist:normalize(chars) for hist, chars in lm.iteritems()}\n",
    "    return outlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this on the Shakespeare text again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxlik_shak = train_char_lm(shakespeare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Now let's do some queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxlik_shak[tuple(list('ello'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxlik_shak[tuple(list('Firs'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating from the model\n",
    "Generating is also very simple. To generate a letter, we will take the history, look at the last $order$ characteters, and then sample a random letter based on the corresponding distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "def generate_letter(lm, history, order):\n",
    "        history = history[-order:]\n",
    "        dist = lm[history]\n",
    "        x = random()\n",
    "        for c,v in dist:\n",
    "            x = x - v\n",
    "            if x <= 0: return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a passage of $k$ characters, we just seed it with the initial history and run letter generation in a loop, updating the history at each turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxlik_generate(lm, order, nletters=1000):\n",
    "    history = [\"<s>\"] * order\n",
    "    out = []\n",
    "    for i in xrange(nletters):\n",
    "        c = generate_letter(lm, tuple(history), order)\n",
    "        history = history[-order:] + [c]\n",
    "        out.append(c)\n",
    "    return \"\".join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Shakespeare from different order models\n",
    "\n",
    "Let's try to generate text based on different language-model orders. Let's start with something silly:\n",
    "\n",
    "### order 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print maxlik_generate(maxlik_shak, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already quite reasonable, and reads like English. Just 4 letters history! What if we increase it to 10?\n",
    "\n",
    "### order 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm10 = train_char_lm(shakespeare, order=10)\n",
    "print maxlik_generate(lm10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This works pretty well\n",
    "\n",
    "With an order of 4, we already get quite reasonable results. Increasing the order to 10 (~two short words of history) already gets us quite passable Shakepearan text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for the fun of it, let's see what our simple language model does with the linux-kernel code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "linux = urllib2.urlopen('http://cs.stanford.edu/people/karpathy/char-rnn/linux_input.txt').read()\n",
    "print('Downloaded text with', len(linux), 'characters')\n",
    "print('Showing the 100 characters in the text...')\n",
    "print(linux[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm = train_char_lm(linux, order=10)\n",
    "print maxlik_generate(lm, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order 10 is pretty much junk. We are far from keeping good indentation and brackets. \n",
    "\n",
    "How could we? we do not have the memory, and these things are not modeled at all. While we could quite easily enrich our model to support also keeping track of brackets and indentation (by adding information such as \"have I seen ( but not )\" to the conditioning history), this requires extra work, non-trivial human reasoning, and will make the model significantly more complex. \n",
    "\n",
    "Let's try this with an LSTM. You may have to run this on your own after class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4900/31034 [===>..........................] - ETA: 480s - loss: 3.7703 - acc: 0.0978"
     ]
    }
   ],
   "source": [
    "linuxX, linuxY, linchar2id = text_to_chardata(linux, 200)\n",
    "print('Data size')\n",
    "print(linuxX.shape, linuxY.shape)\n",
    "linuxmodel = make_model(100, linuxX.shape[1], linuxX.shape[2])\n",
    "linuxmodel.fit(linuxX, linuxY, batch_size = 100, epochs = 15)\n",
    "generate(linuxmodel, 100, linchar2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
